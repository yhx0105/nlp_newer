{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multinomialNB()参数输入的要求是什么？为什么会报错，明天看一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import nltk\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import jieba\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#词统计\n",
    "def make_word_set(word_file):\n",
    "    words_set=set()\n",
    "    with open(word_file,'r',encoding='utf8') as fp:\n",
    "        for line in fp.readlines():\n",
    "            word=line.strip()\n",
    "            if len(word)>0 and word not in words_set:\n",
    "                words_set.add(word)\n",
    "    return words_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#文本 处理，也就是样本生成过程\n",
    "def text_processing(folder_path,test_size=0.2):\n",
    "    folder_list=os.listdir(folder_path)#os.listdir:返回指定路径下的文件和文件夹列表\n",
    "    data_list=[]\n",
    "    class_list=[]\n",
    "    \n",
    "    #遍历文件夹\n",
    "    for folder in folder_list:\n",
    "        new_folder_path=os.path.join(folder_path,folder)#os.path.join:把目录和文件名合成一个路径\n",
    "        files=os.listdir(new_folder_path)\n",
    "        #读取文件\n",
    "        j=1\n",
    "        for file in files:\n",
    "            if j>100:#怕内存爆掉，只取50个样本文件\n",
    "                break\n",
    "            with open(os.path.join(new_folder_path,file),'r',encoding='utf') as fp:\n",
    "                raw=fp.read()\n",
    "            #jieba分词\n",
    "            word_cut=jieba.cut(raw,cut_all=False)#精确模式，返回一个课generate对象\n",
    "            word_list=list(word_cut)#化为list对象，注意编码格式为unicom\n",
    "            \n",
    "            data_list.append(word_list)#训练集list\n",
    "            class_list.append(folder)#类别\n",
    "            j+=1\n",
    "    \n",
    "    #粗暴的划分训练集与测试集合\n",
    "    data_class_list=zip(data_list,class_list)\n",
    "    data_class_list=list(data_class_list)\n",
    "    random.shuffle(data_class_list)\n",
    "    index=int(len(data_class_list)*test_size)+1\n",
    "    train_list=data_class_list[index:]\n",
    "    test_list=data_class_list[:index]\n",
    "    train_data_list,train_class_list=zip(*train_list)\n",
    "    test_data_list,test_class_list=zip(*test_list)\n",
    "    \n",
    "    #统计词频放入all_words_dict\n",
    "    all_words_dict={}\n",
    "    for word_list in train_data_list:\n",
    "        for word in word_list:\n",
    "            if word in all_words_dict:\n",
    "                all_words_dict[word]+=1\n",
    "            else:\n",
    "                all_words_dict[word]=1\n",
    "    \n",
    "    #key函数利用词频进行降序排序\n",
    "    all_words_tuple_list=sorted(all_words_dict.items(),\n",
    "                               key=lambda f:f[1],reverse=True)# 内建函数sorted参数需为list\n",
    "    all_words_list=list(zip(*all_words_tuple_list))[0]\n",
    "    return all_words_list,train_data_list,train_class_list,test_data_list,test_class_list\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_dict(all_words_list,deleteN,stopwords_set=set()):\n",
    "    #选取特征词\n",
    "    feature_words=[]\n",
    "    n=1\n",
    "    for t in range(deleteN,len(all_words_list),1):\n",
    "        if n>1000:#feature_words的维度为50\n",
    "            break\n",
    "        if not all_words_list[t].isdigit() and all_words_list[t] not in stopwords_set and 1<len(all_words_list[t])<5:\n",
    "            feature_words.append(all_words_list[t])\n",
    "            n+=1\n",
    "    print(len(feature_words))\n",
    "    return feature_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#文本特征\n",
    "def text_features(train_data_list,test_data_list,feature_words,flag='sklearn'):\n",
    "    def text_features(text,feature_words):\n",
    "        text_words=set(text)\n",
    "        if flag=='nltk':\n",
    "            #nitk特征dict\n",
    "            features={word:1 if word in text_words\n",
    "                     else 0 for word in feature_words}\n",
    "            \n",
    "        elif flag=='sklearn':\n",
    "            #sklearn特征list\n",
    "            features=[1 if word in text_words else 0\n",
    "                     for word in feature_words]            \n",
    "        else:\n",
    "            features=[] \n",
    "        return features\n",
    "    \n",
    "    train_feature_list=[text_features(text,feature_words)\n",
    "                       for text in train_data_list]\n",
    "    test_feature_list=[text_features(text,feature_words)\n",
    "                      for text in test_data_list]\n",
    "    print(len(train_feature_list))\n",
    "    return train_feature_list,test_feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建朴素贝叶斯模型\n",
    "def text_classifier(train_feature_list,test_feature_list,\n",
    "                   train_class_list,test_class_list,flag='nltk'):\n",
    "    if flag=='nltk':\n",
    "        #使用nltk分类器\n",
    "        train_flist=zip(train_feature_list,train_class_list)\n",
    "        test_flist=zip(test_feature_list,test_class_list)\n",
    "        classifier=nltk.classify.NaiveBayesClassifier.train(train_flist)\n",
    "        test_accuracy=nltk.classify.accuracy(classifier,test_flist)\n",
    "    elif flag=='sklearn':\n",
    "        classifier=MultinomialNB().fit(train_feature_list,train_class_list)#multinomialNB()参数输入的要求是什么？为什么会报错，明天\n",
    "        test_accuracy=classifier.scocre(test_feature_list,test_class_list)\n",
    "    else:\n",
    "        test_accuracy=[]\n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "1000\n",
      "71\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [71, 19]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-181-2cbe82552cf9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m     train_feature_list,test_feature_list=text_features(train_data_list,\n\u001b[0;32m     17\u001b[0m                                                      test_data_list,feature_words,flag)\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mtest_accuracy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtext_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_feature_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_feature_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_class_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_class_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mflag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0mtest_accuracy_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_accuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_accuracy_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-180-5f74e28ac276>\u001b[0m in \u001b[0;36mtext_classifier\u001b[1;34m(train_feature_list, test_feature_list, train_class_list, test_class_list, flag)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mtest_accuracy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_flist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'sklearn'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mclassifier\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMultinomialNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_feature_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_class_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mtest_accuracy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscocre\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_feature_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_class_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    583\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m         \"\"\"\n\u001b[1;32m--> 585\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    764\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    765\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 766\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    767\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    768\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[1;32m--> 235\u001b[1;33m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [71, 19]"
     ]
    }
   ],
   "source": [
    "print ('start')\n",
    "#文本预处理\n",
    "folder_path='./Database/SogouC/Sample'\n",
    "all_words_list,train_data_list,test_data_list,train_class_list,test_class_list=text_processing(folder_path,\n",
    "                                                                                                test_size=0.2)\n",
    "#生产stopwords_set\n",
    "stopwords_file='./stopwords_cn.txt'\n",
    "stopwords_set=make_word_set(stopwords_file)\n",
    "\n",
    "#文本特征提取与分类\n",
    "# flag='sklearn'\n",
    "deleteNs=range(0,1000,20)\n",
    "test_accuracy_list=[]\n",
    "for deleteN in deleteNs:\n",
    "    feature_words=words_dict(all_words_list,deleteN,stopwords_set)\n",
    "    train_feature_list,test_feature_list=text_features(train_data_list,\n",
    "                                                     test_data_list,feature_words,flag)\n",
    "    test_accuracy=text_classifier(train_feature_list,test_feature_list,train_class_list,test_class_list,flag)\n",
    "    test_accuracy_list.append(test_accuracy)\n",
    "print(test_accuracy_list)\n",
    "\n",
    "plt.plot(deleteNs,test_accuracy_list)\n",
    "plt.title('relationship of deleteNs and test_accuracy')\n",
    "plt.xlabel('deleteNs')\n",
    "plt.ylabel('test_accuracy')\n",
    "plt.show()\n",
    "print('finish')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 4), (2, 5), (3, 6)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[1,2,3]\n",
    "b=[4,5,6]\n",
    "c=zip(a,b)\n",
    "list(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
