{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multinomialNB()参数输入的要求是什么？为什么会报错，明天看一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import nltk\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import jieba\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#词统计\n",
    "def make_word_set(word_file):\n",
    "    words_set=set()\n",
    "    with open(word_file,'r',encoding='utf8') as fp:\n",
    "        for line in fp.readlines():\n",
    "            word=line.strip()\n",
    "            if len(word)>0 and word not in words_set:\n",
    "                words_set.add(word)\n",
    "    return words_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#文本 处理，也就是样本生成过程\n",
    "def text_processing(folder_path,test_size=0.2):\n",
    "    folder_list=os.listdir(folder_path)#os.listdir:返回指定路径下的文件和文件夹列表\n",
    "    data_list=[]\n",
    "    class_list=[]\n",
    "    \n",
    "    #遍历文件夹\n",
    "    for folder in folder_list:\n",
    "        new_folder_path=os.path.join(folder_path,folder)#os.path.join:把目录和文件名合成一个路径\n",
    "        files=os.listdir(new_folder_path)\n",
    "        #读取文件\n",
    "        j=1\n",
    "        for file in files:\n",
    "            if j>100:#怕内存爆掉，只取50个样本文件\n",
    "                break\n",
    "            with open(os.path.join(new_folder_path,file),'r',encoding='utf') as fp:\n",
    "                raw=fp.read()\n",
    "            #jieba分词\n",
    "            word_cut=jieba.cut(raw,cut_all=False)#精确模式，返回一个课generate对象\n",
    "            word_list=list(word_cut)#化为list对象，注意编码格式为unicom\n",
    "            \n",
    "            data_list.append(word_list)#训练集list\n",
    "            class_list.append(folder)#类别\n",
    "            j+=1\n",
    "    \n",
    "    #粗暴的划分训练集与测试集合\n",
    "#     data_class_list=zip(data_list,class_list)\n",
    "#     data_class_list=list(data_class_list)\n",
    "# #     print(data_class_list)\n",
    "#     random.shuffle(data_class_list)\n",
    "#     index=int(len(data_class_list)*test_size)+1\n",
    "#     train_list=data_class_list[index:]\n",
    "#     test_list=data_class_list[:index]\n",
    "#     train_data_list,train_class_list=list(zip(*train_list))\n",
    "#     test_data_list,test_class_list=list(zip(*test_list))\n",
    "    \n",
    "    train_data_list, test_data_list, train_class_list, test_class_list = sklearn.cross_validation.train_test_split(data_list, class_list, test_size=test_size)\n",
    "    \n",
    "    #统计词频放入all_words_dict\n",
    "    all_words_dict={}\n",
    "    for word_list in train_data_list:\n",
    "        for word in word_list:\n",
    "            if word in all_words_dict:\n",
    "                all_words_dict[word]+=1\n",
    "            else:\n",
    "                all_words_dict[word]=1\n",
    "    \n",
    "    #key函数利用词频进行降序排序\n",
    "    all_words_tuple_list=sorted(all_words_dict.items(),\n",
    "                               key=lambda f:f[1],reverse=True)# 内建函数sorted参数需为list\n",
    "    all_words_list=list(zip(*all_words_tuple_list))[0]\n",
    "    print(len(all_words_list),len(train_data_list),\n",
    "          len(train_class_list),len(test_data_list),len(test_class_list))\n",
    "    return all_words_list,train_data_list,train_class_list,test_data_list,test_class_list\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_dict(all_words_list,deleteN,stopwords_set=set()):\n",
    "    #选取特征词\n",
    "    feature_words=[]\n",
    "    n=1\n",
    "    for t in range(deleteN,len(all_words_list),1):\n",
    "        if n>1000:#feature_words的维度为50\n",
    "            break\n",
    "        if not all_words_list[t].isdigit() and all_words_list[t] not in stopwords_set and 1<len(all_words_list[t])<5:\n",
    "            feature_words.append(all_words_list[t])\n",
    "            n+=1\n",
    "    print(len(feature_words))\n",
    "    return feature_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#文本特征\n",
    "def text_features(train_data_list,test_data_list,feature_words,flag='sklearn'):\n",
    "    def text_features(text,feature_words):\n",
    "        text_words=set(text)\n",
    "        if flag=='nltk':\n",
    "            #nitk特征dict\n",
    "            features={word:1 if word in text_words\n",
    "                     else 0 for word in feature_words}\n",
    "            \n",
    "        elif flag=='sklearn':\n",
    "            #sklearn特征list\n",
    "            features=[1 if word in text_words else 0\n",
    "                     for word in feature_words]            \n",
    "        else:\n",
    "            features=[] \n",
    "        return features\n",
    "    \n",
    "    train_feature_list=[text_features(text,feature_words)\n",
    "                       for text in train_data_list]\n",
    "    test_feature_list=[text_features(text,feature_words)\n",
    "                      for text in test_data_list]\n",
    "    print((train_feature_list).shape)\n",
    "    return train_feature_list,test_feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建朴素贝叶斯模型\n",
    "def text_classifier(train_feature_list,test_feature_list,\n",
    "                   train_class_list,test_class_list,flag='nltk'):\n",
    "    if flag=='nltk':\n",
    "        #使用nltk分类器\n",
    "        train_flist=zip(train_feature_list,train_class_list)\n",
    "        test_flist=zip(test_feature_list,test_class_list)\n",
    "        classifier=nltk.classify.NaiveBayesClassifier.train(train_flist)\n",
    "        test_accuracy=nltk.classify.accuracy(classifier,test_flist)\n",
    "    elif flag=='sklearn':\n",
    "        classifier=MultinomialNB().fit(train_feature_list,train_class_list)#multinomialNB()参数输入的要求是什么？为什么会报错，明天\n",
    "        test_accuracy=classifier.scocre(test_feature_list,test_class_list)\n",
    "    else:\n",
    "        test_accuracy=[]\n",
    "#     print(test_accuracy)\n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'sklearn' has no attribute 'cross_validation'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-92-ae66ee2ea735>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfolder_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'./Database/SogouC/Sample'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m all_words_list,train_data_list,test_data_list,train_class_list,test_class_list=text_processing(folder_path,\n\u001b[1;32m----> 5\u001b[1;33m                                                                                                 test_size=0.2)\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;31m#生产stopwords_set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mstopwords_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'./stopwords_cn.txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-88-77343ac7df3d>\u001b[0m in \u001b[0;36mtext_processing\u001b[1;34m(folder_path, test_size)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;31m#     test_data_list,test_class_list=list(zip(*test_list))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mtrain_data_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_class_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_class_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_validation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;31m#统计词频放入all_words_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'sklearn' has no attribute 'cross_validation'"
     ]
    }
   ],
   "source": [
    "print ('start')\n",
    "#文本预处理\n",
    "folder_path='./Database/SogouC/Sample'\n",
    "all_words_list,train_data_list,test_data_list,train_class_list,test_class_list=text_processing(folder_path,\n",
    "                                                                                                test_size=0.2)\n",
    "#生产stopwords_set\n",
    "stopwords_file='./stopwords_cn.txt'\n",
    "stopwords_set=make_word_set(stopwords_file)\n",
    "\n",
    "#文本特征提取与分类\n",
    "flag='sklearn'\n",
    "deleteNs=range(0,1000,20)\n",
    "test_accuracy_list=[]\n",
    "for deleteN in deleteNs:\n",
    "    feature_words=words_dict(all_words_list,deleteN,stopwords_set)\n",
    "    train_feature_list,test_feature_list=text_features(train_data_list,\n",
    "                                                     test_data_list,feature_words,flag)\n",
    "    test_accuracy=text_classifier(train_feature_list,test_feature_list,train_class_list,test_class_list,flag)\n",
    "    test_accuracy_list.append(test_accuracy)\n",
    "print(test_accuracy_list)\n",
    "\n",
    "plt.plot(deleteNs,test_accuracy_list)\n",
    "plt.title('relationship of deleteNs and test_accuracy')\n",
    "plt.xlabel('deleteNs')\n",
    "plt.ylabel('test_accuracy')\n",
    "plt.show()\n",
    "print('finish')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-6906292853bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x=(1,2,3)\n",
    "y=[3,2,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
